{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Projects\\\\Alzeimer\\\\Alzheimer-s-Disease-Detection\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from alzheimer_disease.entity import artifacts_entity \n",
    "from alzheimer_disease.entity import config_entity\n",
    "from alzheimer_disease.exception import AlzException\n",
    "from alzheimer_disease.logger import logging \n",
    "from alzheimer_disease.config import ModelManager\n",
    "import sys \n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelEvaluationArtifact:\n",
    "    is_model_accepted:bool\n",
    "    improved_accuracy:float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelEvaluationConfig:\n",
    "        change_threshold :float = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "class ModelEvaluation:\n",
    "    def __init__(self, model_eval_config: artifacts_entity.ModelTrainerArtifcats,\n",
    "                 data_ingestion_artifact: artifacts_entity.DataIngestionArtifact,\n",
    "                 model_trainer_artifact: artifacts_entity.ModelTrainerArtifcats):\n",
    "        try:\n",
    "            logging.info(f\"{'>>'*20}  Model Evaluation {'<<'*20}\")\n",
    "            self.model_eval_config = model_eval_config\n",
    "            self.data_ingestion_artifact = data_ingestion_artifact\n",
    "            self.model_trainer_artifact = model_trainer_artifact\n",
    "            self.model_resolver = ModelManager()\n",
    "        except Exception as e:\n",
    "            raise AlzException(e, sys)\n",
    "\n",
    "    def initiate_model_evaluation(self) -> ModelEvaluationArtifact:\n",
    "        try:\n",
    "            logging.info(\"If the saved model folder has a model, we will compare \"\n",
    "                         \"which model is better trained: the saved model or the current model\")\n",
    "\n",
    "            latest_dir_path = self.model_resolver.get_latest_dir_path()\n",
    "            if latest_dir_path is None:\n",
    "                model_eval_artifact = ModelEvaluationArtifact(is_model_accepted=True,\n",
    "                                                                               improved_accuracy=None)\n",
    "                logging.info(f\"Model evaluation artifact: {model_eval_artifact}\")\n",
    "                return model_eval_artifact\n",
    "\n",
    "            # Load the saved model\n",
    "            model_path = self.model_resolver.get_latest_model_path()\n",
    "            logging.info(\"Loading previously trained model\")\n",
    "            previous_model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "            # Load the current model\n",
    "            logging.info(\"Loading currently trained model\")\n",
    "            current_model = tf.keras.models.load_model(self.model_trainer_artifact.model_dir)\n",
    "\n",
    "            # Evaluate models on the test dataset\n",
    "            test_ds = self.data_ingestion_artifact.test_path\n",
    "            y_true = []\n",
    "            y_pred_previous = []\n",
    "            y_pred_current = []\n",
    "            for images, labels in test_ds:\n",
    "                predictions_previous = previous_model.predict(images)\n",
    "                predicted_labels_previous = tf.argmax(predictions_previous, axis=1).numpy()\n",
    "                y_true.extend(labels.numpy())\n",
    "                y_pred_previous.extend(predicted_labels_previous)\n",
    "\n",
    "                predictions_current = current_model.predict(images)\n",
    "                predicted_labels_current = tf.argmax(predictions_current, axis=1).numpy()\n",
    "                y_pred_current.extend(predicted_labels_current)\n",
    "\n",
    "            # Calculate F1 scores\n",
    "            previous_model_score = f1_score(y_true=y_true, y_pred=y_pred_previous)\n",
    "            current_model_score = f1_score(y_true=y_true, y_pred=y_pred_current)\n",
    "\n",
    "            logging.info(f\"Accuracy using the previously trained model: {previous_model_score}\")\n",
    "            logging.info(f\"Accuracy using the currently trained model: {current_model_score}\")\n",
    "\n",
    "            if current_model_score <= previous_model_score:\n",
    "                logging.info(\"The currently trained model is not better than the previous model\")\n",
    "                raise Exception(\"The currently trained model is not better than the previous model\")\n",
    "\n",
    "            model_eval_artifact = ModelEvaluationArtifact(is_model_accepted=True,\n",
    "                                                                           improved_accuracy=current_model_score - previous_model_score)\n",
    "            logging.info(f\"Model evaluation artifact: {model_eval_artifact}\")\n",
    "\n",
    "            return model_eval_artifact\n",
    "        except Exception as e:\n",
    "            raise AlzException(e, sys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
